# Position
Divided but predominantly critical. While some groups see potential for faster, more consistent decisions, most fear the system will entrench biases, reduce human empathy, and create new barriers for already vulnerable applicants.
## Interests:
- **Fairness and Non-Discrimination**: Ensure the AI does not replicate or amplify existing biases (e.g., against migrants from specific countries, low-income applicants, or those with limited language skills).
- **Transparency and Explainability**: Demand that applicants receive clear, accessible explanations for AI-generated risk scores and how to appeal them.
- **Accessibility**: Advocate for alternatives for applicants who lack digital literacy or reliable internet access, ensuring paper-based applications remain viable.
- **Human Oversight**: Insist that AI recommendations are always reviewed by a human caseworker with the authority to override them.
- **Audits**: Call for external audits by civil society and academic researchers to assess the system’s impact and publish the results. 

## Tensions:
- Between **administration** and **civil society**: The administration’s emphasis on efficiency and modernization clashes with civil society’s demand for slow, inclusive rollouts with strong safeguards.
- Between **legal rights** and **practical realities**: Even if the law guarantees appeals, vulnerable applicants may lack the resources or knowledge to navigate the process, rendering rights theoretical.
- Between **citizens** and **caseworkers**: Citizens may blame caseworkers for AI errors, while caseworkers feel powerless to address systemic flaws in the tool.